{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL REPORT (FR)\n",
    "\n",
    "## Due 29 April 2016 (Friday, 11:59pm)\n",
    "\n",
    "**Important Note:** Before uploading your midterm project on Canvas, please name your file as following:\n",
    "\n",
    "*MT#_FirstLastName.ipynb*\n",
    "\n",
    "where \"#\" denotes the midterm number, \"FirstLastName\" is the name of the student. Students are allowed to work in groups (2 or max. of 3 students). **Each student will hand in their own file**. If you work with another student, please write her/his name on top of the first cell (in a Markdown cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (30 points): CHOOSE YOUR OWN COMPUTATIONAL INTELLIGENCE APPLICATION**\n",
    "\n",
    "In this last exercise, you will choose your own CI application from one of the following main applications (Surrogate-based optimization can be coupled with the other two):\n",
    "\n",
    "* Game AI\n",
    "* 3D Printing\n",
    "* Surrogate-based Optimization\n",
    "\n",
    "You are already familiar with Game AI and 3D printing applications. You can get some ideas about the surrogate-based optimization from the following three papers (you can download them from [UT library](http://www.lib.utexas.edu/) with your EID):\n",
    "\n",
    "* Y. Jin, [A comprehensive survey of fitness approximation in evolutionary computation](http://link.springer.com/article/10.1007%2Fs00500-003-0328-5), Soft Computing, Vol:9, Issue:1, 2005.\n",
    "* A.I.J. Forrester, A.J. Keane, [Recent advances in surrogate-based optimization](http://www.sciencedirect.com/science/article/pii/S0376042108000766), Progress in Aerospace Sciences, Vol:45, 50-79, 2009.\n",
    "* Y. Jin, [Surrogate-assisted evolutionary computation: Recent advances and future challenges](http://www.sciencedirect.com/science/article/pii/S2210650211000198), Swarm and Evolutionary Computation, 61-70, 2011.\n",
    "\n",
    "One of the recent papers that we worked on can be found in this [link](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxjZW1jdHV0dW18Z3g6MmVmY2Q1YjA0ZWVjNzE3MQ).\n",
    "\n",
    "Some other interesting projects could be, but **not limited to**:\n",
    "\n",
    "* Evolutionary multi-objective optimization (EMO) and its applications in games or 3D printing\n",
    "* Evolutionary Many-objective optimization\n",
    "* Use of different evolutionary algorithms: Genetic Programming, Evolution Strategies, Particle Swarm Optimization, Differential Evolution, CMA-ES, Estimation of Distribution Algorithms, etc. (most of these algorithms are avilable in DEAP)\n",
    "* Approximation of 3D objects with cubes, spheres or any base hypercubes using evolutionary algorithms (needs integration of DEAP with OpenSCAD or programming an EA in OpenSCAD)\n",
    "* Designing a 2D car with EAs to finish a given track in shorted amount of time (requires a physics engine)\n",
    "* 3D printable walking or jumping objects (requires a physics engine)\n",
    "* Designing 3D printable accessories using EAs (aesthetic measure is needed for the fitness calculation)\n",
    "* Surrogate-based optimization using a physical simulation or analytical engineering design problem.\n",
    "* Surrogate-based EMO.\n",
    "* Surrogate-based optimization in high-dimensional search space (more than 15 or 20 dimensions).\n",
    "* Robust optimization -- Optimization under uncertainties. For instance, you can investigate the variablity in 3D printing of gears and how to incorporate these variances while designing a reliable gear mechanism\n",
    "* 3D printable lamp design --incorporating variable wall thickness (to control translucency). It may require a physics engine.\n",
    "\n",
    "**IMPORTANT NOTES:** \n",
    "\n",
    "* You can discuss your final project with your friends or mentors, but you have to discuss about it with the instructor before working on it.\n",
    "* Prepare your report in the following format.\n",
    "* Write your report below this cell, not as part of the explanations of format or content.\n",
    "\n",
    "**////////////////////////////////////////////////////////////////**\n",
    "\n",
    "**FORMAT OF THE REPORT:**\n",
    "\n",
    "\n",
    "**Abstract**: \n",
    "\n",
    "* **Briefly** explain the purpose of the exercise, the methodology you followed and the results you obtained. Only one paragraph.\n",
    "\n",
    "**Introduction**:\n",
    "\n",
    "* First paragraph: A short description (3-5 sentences) about your project.\n",
    "* Second paragraph: A *detailed* description of the problem, related work found in the literature. A summary of the structure of your report (what will you be discussing in the upcoming sections?)\n",
    "* Please don't forget to provide any references (**with corresponding numbers**) supporting your sentences.\n",
    "\n",
    "**Methodology**:\n",
    "\n",
    "* You are expected to use an evolutionary algorithm; please provide the details of your implementation (which operators you use?, how do you calculate the fitness values?, etc.)\n",
    "* Please provide the statistics about your calculations.\n",
    "* If you use a simulation, physics engine, analytical test problem or parametric design for 3D printing, give the detailed description of that.\n",
    "* You can provide example figures about the problem.\n",
    "\n",
    "**Results and Discussions**:\n",
    "\n",
    "* You can provide a figure showing the fitness values versus generations. \n",
    "* Results obtained with different algorithm settings.\n",
    "\n",
    "**Conclusions**:\n",
    "\n",
    "* Wrap up your work like you do in Abstract section and provide detailed summary of the results. \n",
    "* If your approach didn't work, you can still give the arguments to help potential readers to avoid the same mistakes.\n",
    "\n",
    "**References**:\n",
    "\n",
    "* List your citations with the order of: Author1, Author2, \"Title of the article\", Name of the Journal or Conference that the paper published at, Pages, Year. \n",
    "* Start your references with numbers and use those numbers in the text.\n",
    "\n",
    "**THANK YOU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# On Minimizing Support Structures Using Surrogate Based Optimization\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Surrogate based optimization is used when evaluating a cost function is computationally expensive. We describe a method by which surrogate based optimization using radial basis function networks (RBF networks) can be used to approximate functions, and apply this method to the problem of finding the optimal planar cut of a 3D model such that the number of support structures is minizimed. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we extend on the idea in MT2 of finding the optimum cutting plane to minimize the number of support structures by extending the search space to all planes in three dimensional space, instead of only focusing on planes parallel to the YZ-plane as we did in MT2. As we stated in MT2, 3D models need support structures if overhangs extend past a 45 degree angle. Support structures preserve the overall shape of the object, but can also affect the appearance and increase cost/time of printing. The main issue with extending this search space is that the time required to compute the number of support structures (the cost function) becomes prohibitively long. Because of this, using techniques like hill climbing or genetic algorithms to find optimal points in this search space is infeasible. The main goal of our project is to approximate the cost function over this large search space using radial basis function networks (RBF networks). \n",
    "\n",
    "Finding the number of support structures for a 3D model is a computationally expensive operation. We timed how long it took to calculate the number of support structures for printing the Colonel 3D object file provided to us and we found that it took around 15 seconds on a modern machine. If we were to use this directly in a genetic algorithm, for instance, the GA may take many hours or days to run depending on parameters such as the population size and the number of design variables. Thus, we need a surrogate model that is faster to compute, while still being accurate. Surrogate-based optimization has been used in a variety of fields, from friction stir welding **[9]** to studying oil reservoir water flooding **[10]**. \n",
    "\n",
    "We attempted to approximate this computationally expensive cost function using radial basis functions, which are much more efficient to compute **[1]**. Radial basis functions are functions that depend on the distance from a center point(hence the term \"radial\")  **[2]**. Some examples of radial basis functions are the Gaussian function and the multiquadric function. In the following examples, we let $r = || x - c ||$, where $x$ is an arbitrary point and $c$ is the center point.\n",
    "\n",
    "Gaussian function:\n",
    "$$\\phi(r) = e^{{- \\epsilon r}^2}$$\n",
    "\n",
    "Multiquadric function:\n",
    "$$\\phi(r) = \\sqrt{1+ (\\epsilon r)^2}$$\n",
    "\n",
    "Radial basis functions have been used before for a variety of applications, ranging from finding solutions of partial differential equations **[5]** and interpolation in surrogate based optimization **[4]**, **[6]**. In **[6]**, radial basis functions are used for \"expensive black-box global optimization\", which is essentially the main goal of this project. In **[6]**, the RBF interpolant is defined as the following. Given $f: V \\to F$, where $V$ is the space of possible inputs and $F$ is a space of scalars, and distinct points $(x_1, f(x_1)), (x_2, f(x_2)), \\ldots, (x_n, f(x_n)) \\in V \\times F$, the interpolant has the following form:\n",
    "\n",
    "$$ s_n(x) = \\sum\\limits_{i=1}^{n} \\lambda_i \\phi(||x_i - c||) + p(x)$$\n",
    "\n",
    "In this formula, $p(x)$ is a polynomial specific to the choice of $\\phi$. For the two RBFs we are studying, which are  the Gaussian function and multiquadric function, $p(x) = b$ and $p(x) = 0$ respectively (where $b$ is a constant). Finding the values of $\\lambda_i$ \"trains\" the RBF network to approximate values of $f$.\n",
    "\n",
    "\n",
    "In our project, we applied RBF interpolation surrogate based optimization to the problem of finding the optimal cut to minimize the number of support structures. We will first discuss the specific methods we used to collect data and how we evaluated the effectiveness of RBF interpolation. We will discuss how we applied RBF optimization to the one-dimensional case, the two-dimensional case, and its effectiveness at higher dimensions.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Plane representation\n",
    "We reused some code from MT2 (namely the code to calculate the number of support structures in slicing.cpp). This code expected six design variables to specify the planar cut: the point of the cut and a normal vector to specify the angle of the cut. We found that we could reduce the number of design variables to just 3 by using Hessian normal form **[7]** to specify the plane. In Hessian normal form, one only needs a unit normal vector, and a distance from the origin to specify an arbitrary plane. We let $(a,b,c)$ be the unit normal vector, and we let $d$ be the distance from the plane to the origin. Since $(a,b,c)$ is a unit vector, we know that $c = \\sqrt{1-a^2-b^2}$.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "For comparison purposes, we first found the global minimum of the cost function. We varied $a$ between -1 and 1 with a step size of 0.2, $b$ from $-\\sqrt{1-a^2}$ and $\\sqrt{1-a^2}$ with a step size of 0.2, and $d$ from $0$ to $100$ with a step size of 10. We found the global minimum to be at the point $(a,b,c) = (0.6, 0.8, 0), d = 70$, with a cost of 16. This is what the cut looks like:\n",
    "\n",
    "<img src=\"http://i.imgur.com/IoAbBIg.jpg\"></img>\n",
    "\n",
    "We found the global minimum because in **[10]**, the main property of a surrogate model that one wants is accuracy in the region of the optimum.  \n",
    "\n",
    "### RBF network implementation\n",
    "\n",
    "We generally used SciPy's RBF network implementation **[8]**, but as an exercise we implemented our own RBF implementation (using some of SciPy's code as a reference). For our RBF network implementation, we used Gaussian radial basis functions, but since SciPy's implementation allows many more types of radial basis functions (including multiquadric functions), we mainly used SciPy's implementation in this project. \n",
    "\n",
    "SciPy's RBF network implementation has two additional parameters that are of particular interest to us. These parameters are called $\\epsilon$ and $\\text{smooth}$. $\\epsilon$ varies the \"influence\" of each radial basis function (see definitions in Introduction), and $\\text{smooth}$ varies the \"smoothness\" of the resulting approximated function. Setting $\\text{smooth} = 0$ will force the approximated function to pass through every point in the training set, which may lead to overfitting.\n",
    "\n",
    "### One dimensional case\n",
    "For the one dimensional case, we varied the $a$ component (and accordingly $c$) of the normal vector with step sizes of 0.2 while keeping $b$ and $l$ constant. We set $b$ and $l$ to 0.8 and 70 respectively, because we wanted to vary $a$ near the global minimum point. After obtaining the data, we constructed two RBF networks with SciPy: one network using Gaussian RBFs and one network using multiquadric RBFs. We experimented with the values of $\\epsilon$ and $\\text{smooth}$ and tried to find the best approximation to our training data, which we measured graphically. To ensure that overfitting was not taking place, we collected new, \"unseen\" data with step sizes of 0.1 (so these points are between the training data points) and tested our RBF model with the unseen data. We tested the viability of our RBF model as a suitable model by running a genetic algorithm to find the global minimum of our model. This genetic algorithm uses the following parameters:\n",
    "\n",
    "* Number of generations: 20\n",
    "* Crossover probability: 0.8\n",
    "* Mutation probability: 0.3\n",
    "* Crossover method: Alpha blending, with $\\alpha=0.01$\n",
    "* Mutation method: Gaussian mutation, with $\\mu=0$, $\\sigma=5$\n",
    "\n",
    "### Two dimensional case\n",
    "For the two dimensional case, we varied $a$ and $b$ with step sizes of 0.2, keeping $l$ fixed at 70. We also constructed a Gaussian RBF network and a multiquadric RBF network, and proceeded with the same methodology as for the one dimensional case.\n",
    "\n",
    "## Results and Discussion\n",
    "\n",
    "### One dimensional case\n",
    "Here we present plots of the approximated function using the training data we collected, which are represented as blue dots. The red dots are \"unseen\" data, and are used to check for overfitting. In the following graphs, the $x$-axis represents the value of $a$ and the $y$-axis represents the resulting cost of the corresponding cut. We show plots for specific values of $\\epsilon$ and $\\text{smooth}$ that show how the RBF model behaves with respect to these parameters.\n",
    "\n",
    "This figure shows the multiquadric RBF function with $\\epsilon=0.1$ and $\\text{smooth}=1$.\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/jd4T718.png\"></img>\n",
    "<caption>Figure 1: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "This model in Figure 1 shows a reasonable approximation to the actual function. We ran our genetic algorithm on this approximated function and we found a minimum cost of 32 at $a=0.57$, which is close to the actual minimum of 16 at $a=0.6$. \n",
    "\n",
    "This figure shows the multiquadric RBF with $\\epsilon=2$ and $\\text{smooth}=1$.\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/UQ2d4AW.png\"></img>\n",
    "<caption>Figure 2: Multiquadric RBF function with $\\epsilon=2$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 2 clearly shows a bad model for both blue and red points. $\\epsilon$ increases the range of the radial basis functions. This means that for a given point along the $x$-axis, it's approximate cost would be more greatly influenced by the actual costs of points relatively far away from the given point. This would decrease the accuracy of the approximated model.\n",
    "\n",
    "This figure shows the multiquadric RBF with $\\epsilon=0.1$, $\\text{smooth}=0$.\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/0Lzzy7d.png\"></img>\n",
    "<caption>Figure 3: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=0$. </caption>\n",
    "</figure>\n",
    "\n",
    "In Figure 3, it is clearly apparent that overfitting is occuring. The function is forced to pass through every blue point. In this graph this still seems to be a good approximation for the red points, but in general this would not be the case. In addition, our genetic algorithm does not seem to work well on this approximated function. It seems to get stuck at a local minimum of 1027.27 at $a=0.117$. Clearly, this not close to the expected minimum at $a=0.6$.\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=0.1$, $\\text{smooth}=1$, which are the same parameters used in the graph shown in Figure 1.\n",
    "\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/jAP04Bj.png\"></img>\n",
    "<caption>Figure 4: Gaussian RBF function with $\\epsilon=0.1$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 4 shows a very poor approximation of the cost function. The oscillating function passes through a few blue points, but not all of them. This approximation passes through no red points.\n",
    "\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=2$ and $\\text{smooth}=1$. \n",
    "\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/GAMmbbV.png\"></img>\n",
    "<caption>Figure 5: Gaussian RBF function with $\\epsilon=2$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 5 is very similar to Figure 2. Both models are not accurate enough for finding the global minimum.\n",
    "\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=0.1$ and $\\text{smooth}=0$. \n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/qj85xbO.png\"></img>\n",
    "<caption>Figure 6: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=0$. </caption>\n",
    "</figure>\n",
    "\n",
    "This is a relatively much better model, but it still suffers from overfitting. One difference that we observed is that our genetic algorithm found the global minimum to be 22.76 at $a=0.5404$, which is closer to the actual global minimum than what was obtained for Figure 3.\n",
    "\n",
    "## Two-dimensional case\n",
    "\n",
    "Here we present 3D plots of the approximated function using the training data we collected, as we did for the one-dimensional case. For clarity, we used green points to represent unseen data and black points to represent training data. The red wireframe surface is our approximated function.\n",
    "\n",
    "\n",
    "This figure shows the multiquadric RBF function with $\\epsilon=0.1$ and $\\text{smooth}=1$.\n",
    "\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/7OqMMbp.png\"></img>\n",
    "<caption>Figure 7: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "It may be hard to see, but this function is a very good approximation of both the training and unseen data. The model does seem to break down as $a$ and $b$ increase, as the approximate cost values are negative. One explanation for this is that this case should never happen in practice, since $b$ is limited to the range $(-\\sqrt{1-a^2}, \\sqrt{1-a^2})$. Therefore, as $a$ increases, $b$ should actually decrease. \n",
    "\n",
    "This figure shows the multiquadric RBF with $\\epsilon=2$ and $\\text{smooth}=1$.\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/xQNDFF6.png\"></img>\n",
    "<caption>Figure 8: Multiquadric RBF function with $\\epsilon=2$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 8 is essentially an extension of Figure 2 into another dimension. We get a bad approximation of both the training data and the unseen data.\n",
    "\n",
    "This figure shows the multiquadric RBF with $\\epsilon=0.1$, $\\text{smooth}=0$.\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/BS15tNE.png\"></img>\n",
    "<caption>Figure 9: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=0$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 9 shows overfitting, similar to Figure 3. Before the function rapidly decreases, it seems to increase for a short interval. This does not reflect the actual value of the function. It seems that the maximum possible cost for slicing Colonel.obj is 2917, when the cut is outside the bounds of the 3D object so nothing is actually cut. Therefore, getting a value above 2917 should not be possible.\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=0.1$, $\\text{smooth}=1$, which are the same parameters used in the graph shown in Figure 7.\n",
    "\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/yzrh6MM.png\"></img>\n",
    "<caption>Figure 10: Gaussian RBF function with $\\epsilon=0.1$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 10 shows an extremely poor approximation of the cost function. The function looks like random noise.\n",
    "\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=2$ and $\\text{smooth}=1$. \n",
    "\n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/fwnWvKP.png\"></img>\n",
    "<caption>Figure 11: Gaussian RBF function with $\\epsilon=2$, $\\text{smooth}=1$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 11 also shows a very poor approximation of the cost function.\n",
    "\n",
    "\n",
    "This figure shows the Gaussian RBF with $\\epsilon=0.1$ and $\\text{smooth}=0$. \n",
    "<figure>\n",
    "<img src=\"http://i.imgur.com/bEZC3yV.png\"></img>\n",
    "<caption>Figure 12: Multiquadric RBF function with $\\epsilon=0.1$, $\\text{smooth}=0$. </caption>\n",
    "</figure>\n",
    "\n",
    "Figure 12 also shows a very bad approximation of the cost function. Though the approximated function passes through every black point, it exhibits overfitting since it does not pass through the green points and varies too much.\n",
    "\n",
    "\n",
    "## Conclusions and Future Work\n",
    "\n",
    "For the one-dimensional case, we found $\\epsilon=0.1$ and $\\text{smooth}=1$ to work well for multiquadric RBF networks, and $\\epsilon=0.1$ and $\\text{smooth}=0$ to work somewhat well for Gaussian RBF networks (which has the overfitting issue). For the two dimensional case, we found all Gaussian RBF networks to be poor approximations while $\\epsilon=0.1$ and $\\text{smooth}=1$ works well for the multiquadric case. We found multiquadric functions to generally be better radial basis functions for approximation (at least in the case of approximating this specific cost function). \n",
    "\n",
    "We were planning on extending this technique to three dimensions, but we did not have enough time to investigate this case thoroughly. If we were successful in approximating the cost function in three dimensions, we could approximate the cost of any arbitrary planar cut in a fraction of the time it would take to compute the exact cost. However, based on preliminary observations, we found the surrogate models in three dimensions to be poor approximations. This may have been caused by a wrong choice of $\\epsilon$ and $\\text{smooth}$.\n",
    "\n",
    "We also planned to write a second \"meta\" genetic algorithm to find the optimum values of $\\epsilon$ and $\\text{smooth}$, which would be a good alternative to essentially trying different values manually until a suitable model is found and may help to produce a better three-dimensional RBF model. This genetic algorithm's fitness function would most likely be the sum of squares difference between the approximated function and the unseen data points. We could also test other kinds of radial basis functions such as the thin-plate spline and the inverse quadratic function.\n",
    "\n",
    "## References (TODO)\n",
    "**[1]** Manuel Kindelan, Miguel Moscoso, \"Radial basis function interpolation in the limit of increasingly flat basis functions\", Journal of Computational Physics, pages 225-242, 2016.\n",
    "\n",
    "**[2]** Martin Buhmann, “Radial Basis Functions: Theory and Implementations”, Cambridge University Press, pages 1-100, 2003.\n",
    "\n",
    "**[3]** D.S. Broomhead, David Lowe, “Multi-Variable Function Interpolation and Adaptive Networks”, Royal Signals & Radar Establishment, pages 1-39, 1988.\n",
    "\n",
    "**[4]** W. Yao, X.Q. Chen, “A surrogate-based optimization method with RBF neural network enhanced by linear interpolation and hybrid infill strategy”, Optimization Methods & Software, pages 406-429, 2014.\n",
    "\n",
    "**[5]** Bengt Fornberg, Julia Zuev, “The Runge phenomenon and spatially variable shape parameters in RBF interpolation”, Computers and Mathematics with Applications, pages 379-398, 2007.\n",
    "\n",
    "**[6]** Kenneth Holmstrom, “An adaptive radial basis algorithm (ARBF) for expensive black-box global optimization”, Journal of Global Optimization, pages 447-464, 2008.\n",
    "\n",
    "**[7]** http://mathworld.wolfram.com/HessianNormalForm.html\n",
    "\n",
    "**[8]** https://www.scipy.org\n",
    "\n",
    "**[9]** Cem C. Tutum, Shaayaan Sayed, Risto Miikkulainen, “Surrogate-based Evolutionary Optimization for Friction Stir Welding”, Proceedings of IEEE World Congress on Computational Intelligence (WCCI 2016), 8 pages, 2016.\n",
    "\n",
    "**[10]** Eka Suwartadi, Stein Krogstad, “Adjoint-based surrogate optimization of oil reservoir water flooding”, Optimization and Engineering, pages 441-481, 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
